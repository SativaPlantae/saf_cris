import os
import streamlit as st
import pandas as pd
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.vectorstores import FAISS
from langchain.embeddings import OpenAIEmbeddings
from langchain.chat_models import ChatOpenAI
from langchain.chains import ConversationalRetrievalChain
from langchain.memory import ConversationBufferMemory
from langchain.prompts import PromptTemplate
from langchain.schema import Document

# ğŸ” Chave da OpenAI
openai_api_key = os.getenv("OPENAI_API_KEY")

# ğŸ”„ Carrega planilha e configura cadeia com memÃ³ria
@st.cache_resource
def carregar_chain_com_memoria():
    # Carregando dados do SAF Cristal
    df = pd.read_csv("data.csv")
    texto_unico = "\n".join(df.astype(str).apply(lambda x: " | ".join(x), axis=1))
    document = Document(page_content=texto_unico)

    splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
    docs = splitter.split_documents([document])

    embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)
    vectorstore = FAISS.from_documents(docs, embeddings)
    retriever = vectorstore.as_retriever()

    # Prompt com estilo natural
    prompt_template = PromptTemplate(
        input_variables=["chat_history", "context", "question"],
        template="""
VocÃª Ã© um assistente virtual treinado com base em uma planilha tÃ©cnica sobre o Sistema Agroflorestal SAF Cristal.

Fale de forma clara, didÃ¡tica e acessÃ­vel, como se estivesse conversando com um estudante ou alguÃ©m curioso. Use o histÃ³rico da conversa para manter a fluidez. Evite respostas robÃ³ticas. Se nÃ£o tiver certeza, diga isso de forma sutil e humana.

-------------------
HistÃ³rico:
{chat_history}

InformaÃ§Ãµes encontradas:
{context}

Pergunta: {question}
Resposta:"""
    )

    memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)

    chain = ConversationalRetrievalChain.from_llm(
        llm=ChatOpenAI(model="gpt-4o-mini", temperature=0.5, openai_api_key=openai_api_key),
        retriever=retriever,
        memory=memory,
        combine_docs_chain_kwargs={"prompt": prompt_template}
    )

    return chain

# âš™ï¸ ConfiguraÃ§Ã£o visual
st.set_page_config(page_title="Chatbot SAF Cristal ğŸŒ±", page_icon="ğŸŒ¬")
st.title("ğŸŒ¬ Chatbot do SAF Cristal")
st.markdown("Converse com o assistente sobre o Sistema Agroflorestal Cristal ğŸ“Š")

# Inicializa o histÃ³rico visual (mensagens)
if "mensagens" not in st.session_state:
    st.session_state.mensagens = []

# Inicializa a cadeia com memÃ³ria
if "qa_chain" not in st.session_state:
    st.session_state.qa_chain = carregar_chain_com_memoria()

# ExibiÃ§Ã£o do histÃ³rico
for remetente, mensagem in st.session_state.mensagens:
    with st.chat_message("user" if remetente == "ğŸ§‘â€ğŸŒ¾" else "assistant"):
        st.markdown(mensagem)

# Input no final
user_input = st.chat_input("Digite sua pergunta aqui...")

if user_input:
    # Exibe pergunta do usuÃ¡rio
    with st.chat_message("user"):
        st.markdown(user_input)
    st.session_state.mensagens.append(("ğŸ§‘â€ğŸŒ¾", user_input))

    # Gera resposta
    with st.spinner("Consultando o SAF Cristal..."):
        try:
            resposta = st.session_state.qa_chain.run(user_input)
        except Exception as e:
            resposta = f"âš ï¸ Ocorreu um erro: {e}"

    # Exibe resposta do bot
    with st.chat_message("assistant"):
        st.markdown(resposta)
    st.session_state.mensagens.append(("ğŸŒ¬", resposta))
