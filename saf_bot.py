import os
import streamlit as st
import pandas as pd
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.vectorstores import FAISS
from langchain.embeddings import OpenAIEmbeddings
from langchain.chat_models import ChatOpenAI
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
from langchain.schema import Document

# ğŸ” Chave da OpenAI
openai_api_key = os.getenv("OPENAI_API_KEY")

@st.cache_resource
def carregar_qa_chain():
    # 1. Carregando os dados do SAF Cristal
    df = pd.read_csv("data.csv")
    texto_unico = "\n".join(df.astype(str).apply(lambda x: " | ".join(x), axis=1))

    # 2. Transformando em documento LangChain
    document = Document(page_content=texto_unico)

    # 3. Split
    splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
    docs = splitter.split_documents([document])

    # 4. VetorizaÃ§Ã£o
    embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)
    vectorstore = FAISS.from_documents(docs, embeddings)
    retriever = vectorstore.as_retriever()

    # 5. Prompt mais natural, sem repetiÃ§Ã£o robÃ³tica
    prompt_template = PromptTemplate(
        input_variables=["context", "question"],
        template="""
VocÃª Ã© um assistente virtual treinado com base em uma planilha de dados tÃ©cnicos sobre o Sistema Agroflorestal SAF Cristal.

Sua linguagem Ã© didÃ¡tica, amigÃ¡vel e acessÃ­vel, como se estivesse conversando com um estudante ou profissional da Ã¡rea ambiental. Evite repetiÃ§Ãµes, formalismos excessivos e frases robÃ³ticas.

Caso nÃ£o tenha certeza da resposta, seja transparente e diga isso de forma leve, como:
- "NÃ£o tenho certeza absoluta, mas posso te contar o que aparece por aqui."
- "Parece que esse detalhe nÃ£o estÃ¡ explÃ­cito, mas vamos lÃ¡..."
- "NÃ£o encontrei exatamente isso, mas com base no que sei, diria que..."

-------------------
{context}

Pergunta: {question}
Resposta:"""
    )

    # 6. Modelo LLM
    llm = ChatOpenAI(
        model="gpt-4o-mini",
        temperature=0.5,
        max_tokens=500,
        openai_api_key=openai_api_key
    )

    qa_chain = RetrievalQA.from_chain_type(
        llm=llm,
        retriever=retriever,
        chain_type="stuff",
        chain_type_kwargs={"prompt": prompt_template}
    )

    return qa_chain

# ğŸŒ Interface do app
st.set_page_config(page_title="Chatbot SAF Cristal - Sativa Plantae", page_icon="ğŸŒ¬")
st.title("ğŸŒ¬ Chatbot do SAF Cristal")
st.markdown("Converse com o assistente sobre o Sistema Agroflorestal Cristal ğŸ“Š")

# HistÃ³rico de conversa
if "mensagens" not in st.session_state:
    st.session_state.mensagens = []

qa_chain = carregar_qa_chain()

# FormulÃ¡rio de envio
with st.form(key="formulario_chat", clear_on_submit=True):
    user_input = st.text_input("ğŸ§‘â€ğŸŒ¾ VocÃª:", placeholder="Pergunte algo sobre o SAF Cristal...")
    submit = st.form_submit_button("Enviar")

# Processamento
if submit and user_input:
    with st.spinner("Consultando o SAF..."):
        try:
            resposta = qa_chain.run(user_input)
            st.session_state.mensagens.append(("ğŸ§‘â€ğŸŒ¾", user_input))
            st.session_state.mensagens.append(("ğŸŒ¬", resposta))
        except Exception as e:
            st.error(f"Ocorreu um erro: {e}")

# ExibiÃ§Ã£o da conversa completa
for remetente, mensagem in st.session_state.mensagens:
    st.markdown(f"**{remetente}**: {mensagem}")
